{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11d84ab00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading Data \n",
    "mat_contents = sio.loadmat('./Data/binaryalphadigs.mat')\n",
    "plt.imshow(mat_contents['dat'][3][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lire_alphaDigits(class_labels, all_data):\n",
    "    \"\"\"\n",
    "        The following function aims to extract from our dataset the part corresponding the class_labels that we \n",
    "        wish to extract. It also help put in order the output matrix to be directly used with our algorithm.\n",
    "        \n",
    "        ---\n",
    "        Parameters:\n",
    "            class_labels : n_array (n, )\n",
    "                            A vector containing the labels that we wish to extract\n",
    "            all_data : matrix\n",
    "                            Our data set (In the form of a Dictionnary -- extracted from a Matlab Matrix(.mat))\n",
    "        \n",
    "        ---\n",
    "        Attributes:\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    final = []\n",
    "    for elements in class_labels:\n",
    "        temp = np.array([all_data[elements][i].flatten() for i in range(len(all_data[elements]))])\n",
    "        final.append(temp)\n",
    "\n",
    "    return np.concatenate(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 100\n",
    "p = 320\n",
    "class RBM():\n",
    "    \n",
    "    \"\"\"\n",
    "    This class represents our Restricted Boltzman Machine Structure. It contains the differents weights and biais\n",
    "    for both layers that we will be using. \n",
    "    \n",
    "    \n",
    "    ---\n",
    "    Parameters:\n",
    "        p : int (1,)\n",
    "            The number of neurons in the first layer representing the Latent variables\n",
    "            \n",
    "        q : int (1,)\n",
    "            The number f neurons in the second layer representing the Hidden variables\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, p, q):\n",
    "        self.W = np.random.normal(0, 0.01, (p, q))\n",
    "        self.a = np.zeros(p)\n",
    "        self.b = np.zeros(q)\n",
    "        \n",
    "    def update_values(self, val):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function aims to help updating the different parameters during the training part. \n",
    "        The user have to give as an input a vector of values to be updated (In our case, it's gonna be \n",
    "        the gradient)\n",
    "        \n",
    "        --- \n",
    "        Parameters :\n",
    "            val : vector (3,)\n",
    "                    Containing the correspondant value to be updated (The gradient in respect to each parameter)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.W += val[0]\n",
    "        self.a += val[1]\n",
    "        self.b += val[2]\n",
    "        \n",
    "    def print(self):\n",
    "        print(self.W, self.a, self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entree_sortie_RBM(RBM, donnee):\n",
    "    \"\"\"\n",
    "    This function returns the output value of each part of our layer.\n",
    "    We recall that we have found in the theoritical computation that : \n",
    "    P(h_j = 1 | v) = sigm(b_j + \\sum{w_{i,j} v_i}) with v being our observed variables and h our latent variables\n",
    "    and w the matrix of weights and the biais of the first output layer.\n",
    "    \n",
    "    ---\n",
    "    Parameters:\n",
    "        RBM : class\n",
    "                Our RBM structure that was previously coded as our class\n",
    "        donnee : \n",
    "                Our data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sortie = 1 / (1 + np.exp(- RBM.b.reshape(1,-1) - donnee @ RBM.W ))\n",
    "    \n",
    "    return sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortie_entree_RBM(RBM, donnee):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the output value of each part of our layer.\n",
    "    We recall that we have found in the theoritical computation that : \n",
    "    P(v_i = 1 | h) = sigm(a_i + \\sum{w_{i,j} h_j}) with v being our observed variables and h our latent variables\n",
    "    and w the matrix of weights and the biais of the first output layer.\n",
    "    \n",
    "    ---\n",
    "    Parameters:\n",
    "        RBM : class\n",
    "                Our RBM structure that was previously coded as our class\n",
    "        donnee : \n",
    "                Our data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    entree = 1 / (1 + np.exp(- RBM.a.reshape(1,-1) - donnee @ RBM.W.T))\n",
    "                  \n",
    "    return entree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RBM(RBM, x, iter_gradient, epsilon, batch_size, verbose = False):\n",
    "    \n",
    "    \"\"\"\n",
    "        This function aims to compute the training part of our algorithm. \n",
    "        We recall that the goal is to maximize our Maximum-Vraisemblance, we first start by computing the gradient\n",
    "        in respect to each parameter and then use an optimization approch to update our parameters. \n",
    "        \n",
    "        We will be using the Gibbs Sampling approach to estimate the second term of the gradient which is related\n",
    "        to the expectancy. \n",
    "        \n",
    "        \n",
    "        ---\n",
    "        Parameters:\n",
    "            RBM :\n",
    "                    Our RBM structure\n",
    "            x : matrix (num_of_samples, p)\n",
    "                    Our Observed variables (It will be the result of the lire_alphaDigits function \n",
    "                    that was previously coded)\n",
    "                \n",
    "            iter_gradient: int (1, )\n",
    "                    The number of iteration during the optimization process\n",
    "            \n",
    "            epsilon : int (1, )\n",
    "                    The learning rate \n",
    "                    \n",
    "            batch_size : int (1,)\n",
    "                    The batch size to be used during the training\n",
    "                    \n",
    "            verbose : BOOL\n",
    "                    Parameter controlling weather the user want to output the values during the optimization part.\n",
    "            \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    p,q = RBM.W.shape\n",
    "    n = x.shape[0]\n",
    "    for i in range(iter_gradient):\n",
    "        x_copy = x.copy()\n",
    "        np.random.shuffle(x_copy)\n",
    "        \n",
    "        for batch in range(0, n, batch_size):\n",
    "            \n",
    "            v_0 = x_copy[batch : min(batch + batch_size, n), :]\n",
    "            h_0 = (np.random.uniform(size = (len(v_0), q)) < entree_sortie_RBM(RBM, v_0)).astype('float')\n",
    "            v_1 = (np.random.uniform(size = (len(v_0), p)) < sortie_entree_RBM(RBM, h_0)).astype('float')\n",
    "            \n",
    "            # Gradient compilation\n",
    "            d_a = np.sum(v_0 - v_1, axis = 0)\n",
    "            d_b = np.sum(entree_sortie_RBM(RBM, v_0) - entree_sortie_RBM(RBM, v_1), axis = 0)\n",
    "            d_W = np.dot(v_0.T, entree_sortie_RBM(RBM, v_0)) - np.dot(v_1.T, entree_sortie_RBM(RBM, v_1))\n",
    "            \n",
    "            cst = epsilon/len(v_0)\n",
    "            RBM.update_values([cst * d_W, cst * d_a, cst *d_b])\n",
    "        \n",
    "        sortie = entree_sortie_RBM(RBM, x_copy)\n",
    "        new_data = sortie_entree_RBM(RBM, sortie)\n",
    "        erreur = np.linalg.norm(x_copy - new_data, ord='fro')**2 / x_copy.size\n",
    "        if verbose:\n",
    "            print(erreur)\n",
    "    return RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_RBM(RBM, nb_images, nb_iter):\n",
    "    \n",
    "    \"\"\"\n",
    "    Using our RBM, the following function uses the different weights that were already updated to generate new \n",
    "    samples. \n",
    "    \n",
    "    ---\n",
    "    Parameters:\n",
    "        RBM : \n",
    "                Our RBM class with the weights already trained and updated\n",
    "        nb_images : int (1,)\n",
    "                The number of samples that we want to generate\n",
    "                \n",
    "        nb_iter : int (1,)\n",
    "                The number of iterations used during the generation\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    p,q = RBM.W.shape\n",
    "    for i in range(nb_images):\n",
    "        x = (np.random.uniform(size=p) < 0.5).astype('float')\n",
    "        \n",
    "        for iter in range(nb_iter):\n",
    "            h = (np.random.uniform(size = q) < entree_sortie_RBM(RBM, x)).astype('float')\n",
    "            x = (np.random.uniform(size = p) < sortie_entree_RBM(RBM, h)).astype('float')\n",
    "        \n",
    "        x =  np.reshape(x, (20, 16))\n",
    "        plt.imshow(x, cmap = 'gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = lire_alphaDigits([2], mat_contents['dat'])\n",
    "iter_gradient = 1000\n",
    "epsilon = 0.1\n",
    "batch_size = 3\n",
    "rbm = RBM(320, 100)\n",
    "rbm = train_RBM(rbm, x, iter_gradient, epsilon, batch_size, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_image_RBM(rbm, 3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RBM Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RBM import RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training successfully over.\n"
     ]
    }
   ],
   "source": [
    "x = lire_alphaDigits([2], mat_contents['dat'])\n",
    "iter_gradient = 1000\n",
    "epsilon = 0.1\n",
    "batch_size = 3\n",
    "rbm = RBM(320, 100)\n",
    "rbm.fit(x, iter_gradient, epsilon, batch_size, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAN0AAAEICAYAAADbQPEyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAE1BJREFUeJzt3Xu0nFV9xvHvQwIKJGpoMEISLqUpNtB6lBi8gA0FMWTRRisLw6o2Kha1slpXsYp1LbBoW6yliAWBQGPAhYCtRlONQEqLQpFLQsNVkJRbzjFXwiUIlh749Y93HxwmMydzZubsmXfm+ax11nnnve53Zp55L7Nnb0UEZpbPLp0ugFm/cejMMnPozDJz6Mwyc+jMMnPozDJz6LqQpA9KuqnOtP0kPSNpQu5ydStJyyR9sdPlaFRpQidpkaRbJf1C0uY0/KeS1OmyVZN0g6SPjMe6I+KxiJgUES+Mx/ptR5JOlHSzpGcl3dDq+koROkmnAecBXwZeB0wDPga8Hdgtc1km5tye5VXnDGIb8BXg7LZsJCK6+g94NfAL4L07me8VwD8AjwGbgIuA3dO0ecAgcBqwGdgAfGiMy34G2Ah8A5gCfB/YAjyRhmek+f8GeAH4JfAMcH4a/3pgVXoBHwBOrNj+rwErgKeB24AvADfV2c8DgAAmpsc3AF8Ebk7b+7e0vivS+m4HDqhY/jxgfZq2BjiyYtruwGVpn34KfBoYrJi+L/DttN8PA382yuuxDLgA+AGwHbgVOKjWPlTsx0fS8AeB/wLOBZ4EHgLelsavT6/h4qptXZSe3+3Aj4D9K6aP9twvAy4EVlK8z44ZZZ8+AtzQ8nu606FqIHTzgeHKF6jOfOemN+5ewOT05vu7iuAMA2cBuwILgGeBKWNY9ksU4dw9vanfC+yR5v8X4Lu13kDp8Z7pzfIhYCLwRmArMDtNvwr4VprvUGCIsYVuHXAQxQfUfcDPgGPSti4Hvl6x/PtT+SdSfAhtBF6Zpp2d3rBTgBnAXaTQUZwVrQHOoDi7+PUUhneNErrHgblpW1cAV40hdMPp+ZpA8aHyGEWIXwEcSxGuSRXb2g68I00/b+T5a+C5XwY8RXHWtMvIc9HvoXs/sLFq3M0Un4DPpSdaFJ9SB1XM81bg4YrgPFf1Im8G3tLgss/v5MUYAJ4YJXTvA26sWuZi4Mz0pvo/4PUV0/6WsYXucxXTzwF+WPH494G1o5T9CeANafhlIUpvspHQHQ48VrXsZ6kIdI3QXVrxeAFw/xhC92DFtN9O80+rGPc4MFCxrasqpk2iONuYOdpzX7Hs5Q2+F9sSujJcnzwOTJU0MSKGASLibQCSBik+nfamOOqsqbivIoo39EvrGVk+eZbixWlk2S0R8cuXJkp7UBwd51McFQAmS5oQtW9w7A8cLunJinETKU5V907D6yumPVr7qahrU8XwczUeT6oo+6eAkylOFQN4FTA1Td63qhyVw/sD+1btwwTgxlHKtbFieOT5blT1PhARdfersqwR8YykbRT7M9pzv8OyOZQhdD8B/hdYSHE9UctWihfhkIgYGuP6G1m2+qcYpwEHA4dHxEZJA8B/U4S11vzrgR9FxDurV5wu3IcpPpXvT6P3G+M+NETSkRTXaUcD90bEi5KeqCj3BorTyvvS45kVi6+nOPrPakNRfpH+70FxbQnFDbJWvFRWSZMoLhV+zijPfYWsP7Xp+ruXEfEk8NfA1ySdIGmypF3SG33PNM+LwCXAuZJeCyBpuqR3NbD+ZpadTBHUJyXtRXGaWGkTxTXPiO8DvynpA5J2TX9vlvRb6cj4HeDzkvaQNBtYvLNyN2kyRcC3ABMlnUFxpBvxLeCzkqZImg6cWjHtNmC7pM9I2l3SBEmHSnrzWAsREVsorlvfn9bzYYpr0lYskHSEpN0obkTdEhHrGeW5b3TFqYyvpDhI7SLplZJ2bbagXR86gIj4e+AvKD6lN6W/iynuKN6cZvsMxQ2FWyQ9Dfw7xdGoEWNd9isUN1S2ArcA11RNPw84QdITkr4aEdspLv4XUXz6buRXN2ageHNPSuOXAV9vsNxjdW0q688oTmF/yctPrc6iuFP7MMVz8K8UZxmkD4fjKa5fH6bY90spbt4040+Av6S4fDiEX72OzfomxYffNuAwinsBNPDcN+IDFB+yFwJHpuFLmi2o0gWi2Q4kfRxYFBG/2+my9JJSHOksD0n7SHp7On0/mOLadXmny9VrynAjxfLZjeK0/UCKr2SuAr7W0RL1IJ9emmXm00uzzLry9FJSaQ6/hx12WKeLwJo1azpdBCtsjYi9dzZTV55elil03fD8deGvm/rVmoiYs7OZfHpplllLoZM0X9IDktZJOr3G9FdIujpNv1XSAa1sz6wXNB26VGfwAuA4YDZwUqrCVOlkitr3v0FRQfhLzW7PrFe0cqSbC6yLiIci4nmK73QWVs2zkOJHkVBUKTq6G5tXMMupldBN5+X19gbTuJrzpJ/VPEXxA8odSDpF0mpJq1sok1nX65qvDCJiCbAEynX30mysWjnSDfHy31vNSONqzpMa9Hk1Ra1ys77VSuhuB2ZJOjD9hmkRRTsjlVbwq9+GnQD8R3TDF1tmHdT06WVEDEs6leI3WhOApRFxr6SzgNURsQL4Z+AbktZR/M5pUTsKbVZmpa+R0o3l70e+KQ24RopZd3LozDJz6Mwyc+jMMnPozDJz6Mwyc+jMMnPozDJz6Mwyc+jMMuuan/ZYuY2lOl6/Vxnzkc4sM4fOLDOHziwzh84sM4fOLDOHziwzh84ss1ZaeJ4p6T8l3SfpXkl/XmOeeZKekrQ2/Z3RWnHNyq+VL8eHgdMi4g5Jk4E1klZFxH1V890YEce3sB2zntL0kS4iNkTEHWl4O/BTdmzh2cyqtOWaLvXG80bg1hqT3yrpTkk/lHTIKOt4qVn1/fbbj4ho6M/Kp99f25ZDJ2kS8G3gkxHxdNXkO4D9I+INwD8B3623nohYEhFzImLO3nvvtDNLs9JqtX+6XSkCd0VEfKd6ekQ8HRHPpOGVwK6SprayTbOya+XupShacP5pRPxjnXleN9I1lqS5aXvuy8D6Wit3L98OfAC4W9LaNO6vgP0AIuIiiv4LPi5pGHgOWOS+DKzftdKXwU3AqD+MiojzgfOb3YZZL3KNFLPMHDqzzBw6s8wcOrPMHDqzzBw6s8wcOrPMHDqzzBw6s8wcOrPMHDqzzBw6s8wcOrPMHDqzzBw6s8zcP511rV7t885HOrPMHDqzzNrRBN8jku5OzaavrjFdkr4qaZ2kuyS9qdVtmpVZu67pjoqIrXWmHQfMSn+HAxem/2Z9Kcfp5ULg8ijcArxG0j4ZtmvWldoRugCuk7RG0ik1pk8H1lc8HqRGnweVzapv2bKlDcUy607tCN0REfEmitPIT0h6RzMrcbPq1i9aDl1EDKX/m4HlwNyqWYaAmRWPZ6RxZn2p1b4M9kx90yFpT+BY4J6q2VYAf5zuYr4FeCoiNrSyXbMya/Xu5TRgeaoNMBH4ZkRcI+lj8FLT6iuBBcA64FngQy1u06zUWgpdRDwEvKHG+IsqhgP4RCvb6WZlqn4EY6taZePDNVLMMnPozDJz6Mwyc+jMMnPozDJz6Mwyc+jMMnPozDJz6Mwyc+jMMlM3VguS1HChxqv8ZaveNR668b1RT5e8XmsiYs7OZvKRziwzh84sM4fOLDOHziwzh84sM4fOLDOHziyzpkMn6eDUlPrI39OSPlk1zzxJT1XMc0brRTYrt6bbSImIB4ABAEkTKJrVW15j1hsj4vhmt2PWa9p1enk08D8R8Wib1mfWs9rVgcgi4Mo6094q6U7g58CnIuLeWjOlJtlrNcs+qi6p/mPjoFdf25brXkrajSJQh0TEpqpprwJejIhnJC0AzouIWQ2sszyV/npYp+teljB02epeHgfcUR04gIh4OiKeScMrgV0lTW3DNs1Kqx2hO4k6p5aSXqf0cSVpbtre423YpllptXRNl/oveCfw0YpxlU2qnwB8XNIw8BywKDp9zmLWYaX/PZ2Nn06/N3xNZ2Zt4dCZZebQmWXm0Jll5tCZZdauamBWEp2+I2k+0pll59CZZebQmWXm0Jll5tCZZebQmWXm0Jll5tCZZebQmWXm0Jll5mpgPcBVu8rFRzqzzBoKnaSlkjZLuqdi3F6SVkl6MP2fUmfZxWmeByUtblfBzcqq0SPdMmB+1bjTgetTO5bXp8cvI2kv4EzgcGAucGa9cJr1i4ZCFxE/BrZVjV4IXJaGLwPeXWPRdwGrImJbRDwBrGLH8Jr1lVau6aZFxIY0vBGYVmOe6cD6iseDaZxZ32rL3cuIiFabzWu2LwOzsmnlSLdJ0j4A6f/mGvMMATMrHs9I43YQEUsiYk4j7QaalVkroVsBjNyNXAx8r8Y81wLHSpqSbqAcm8aZ9a1GvzK4EvgJcLCkQUknA2cD75T0IHBMeoykOZIuBYiIbcAXgNvT31lpnFnfcrPqPaAbX8N26NVm1V0NrEv1apCglGFqK1cDM8vMoTPLzKEzy8yhM8vMoTPLzKEzy8yhM8vMoTPLzKEzy8yhM8vM1cDqaLQaVr9XaRrh56FxPtKZZebQmWXm0Jll5tCZZebQmWXm0Jll5tCZZbbT0NXpx+DLku6XdJek5ZJeU2fZRyTdLWmtpNXtLLhZWTVypFvGjk2hrwIOjYjfAX4GfHaU5Y+KiAG3Z2lW2GnoavVjEBHXRcRwengLRSOyZtaAdlQD+zBwdZ1pAVyXmtS7OCKW1FtJtzWrPh7Vmnq5ha+x6PTz0Okqay2FTtLngGHgijqzHBERQ5JeC6ySdH86cu4gBXJJWq/fndazmr57KemDwPHAH0Wdj66IGEr/NwPLKfqoM+trTYVO0nzg08AfRMSzdebZU9LkkWGKfgzuqTWvWT9p5CuDWv0YnA9MpjhlXCvpojTvvpJWpkWnATdJuhO4DfhBRFwzLnthViLuyyCjbnyu22UsNyc6/TyM442UhvoycI0Us8wcOrPMHDqzzBw6s8wcOrPM3BpYizp9J65blOl5GK+yNnpX1Ec6s8wcOrPMHDqzzBw6s8wcOrPMHDqzzBw6s8wcOrPMHDqzzFwjpY4y1bAYL2X6jVyZ+EhnlplDZ5ZZs82qf17SUGofZa2kBXWWnS/pAUnrJJ3ezoKblVWzzaoDnJuaSx+IiJXVEyVNAC4AjgNmAydJmt1KYc16QVPNqjdoLrAuIh6KiOeBq4CFTazHrKe0ck13auq1Z6mkKTWmTwfWVzweTONqknSKpNXu3cd6XbOhuxA4CBgANgDntFqQiFgSEXPcu4/1uqZCFxGbIuKFiHgRuITazaUPATMrHs9I48z6WrPNqu9T8fA91G4u/XZglqQDJe0GLAJWNLM9s16y0xopqVn1ecBUSYPAmcA8SQMUXWE9Anw0zbsvcGlELIiIYUmnAtcCE4ClEXHvuOyFWYn0VbPq3biv1jskuVl1s27k0Jll5tCZZebQmWXm0Jll5tCZZebQmWXm0Jll5tCZZebQmWVW+tbAXLWrfBptZaxXX1sf6cwyc+jMMnPozDJz6Mwyc+jMMnPozDJz6Mwya6SNlKXA8cDmiDg0jbsaODjN8hrgyYgYqLHsI8B24AVg2M3rmTX25fgy4Hzg8pEREfG+kWFJ5wBPjbL8URGxtdkCmvWanYYuIn4s6YBa01RULTgR+L32Fsusd7VaDexIYFNEPFhnegDXpda9Lo6IJfVWJOkU4JSxFsAdF5ZPp1+HsbxnxkOroTsJuHKU6UdExJCk1wKrJN2fOiTZQQrkEhi/JvjMukHTdy8lTQT+ELi63jwRMZT+bwaWU7v5dbO+0spXBscA90fEYK2JkvaUNHlkGDiW2s2vm/WVRnpivRL4CXCwpEFJJ6dJi6g6tZS0r6SRDiKnATdJuhO4DfhBRFzTvqKblZObVbe+M443Utysulk3cujMMnPozDJz6Mwyc+jMMit9a2BWPp2uhtVpPtKZZebQmWXm0Jll5tCZZebQmWXm0Jll5tCZZebQmWXm0Jll5tCZZdat1cC2Ao9WjZuaxjetS6sftbxfXaxX963efu3fyMJd+cvxWiSt7sUWont1v6B3963V/fLppVlmDp1ZZmUKXd3WoUuuV/cLenffWtqv0lzTmfWKMh3pzHqCQ2eWWSlCJ2m+pAckrZN0eqfL0y6SHpF0t6S1klZ3ujytkLRU0mZJ91SM20vSKkkPpv9TOlnGZtTZr89LGkqv21pJC8ayzq4PnaQJwAXAccBs4CRJsztbqrY6KiIGeuD7rGXA/KpxpwPXR8Qs4Pr0uGyWseN+AZybXreBiFhZY3pdXR86ip5+1kXEQxHxPHAVsLDDZbIqqQu0bVWjFwKXpeHLgHdnLVQb1NmvlpQhdNOB9RWPB9O4XjDSaeaa1Clmr5kWERvS8EaKTmV6xamS7kqnn2M6bS5D6HrZERHxJopT509IekenCzReovhuqle+n7oQOAgYADYA54xl4TKEbgiYWfF4RhpXen3QaeYmSfsApP+bO1yetoiITRHxQkS8CFzCGF+3MoTudmCWpAMl7UbRL96KDpepZX3SaeYKYHEaXgx8r4NlaZuRD5LkPYzxdevWn/a8JCKGJZ0KXAtMAJZGxL0dLlY7TAOWp58bTQS+WeZOM1PnofOAqZIGgTOBs4FvpY5EHwVO7FwJm1Nnv+ZJGqA4XX4E+OiY1ulqYGZ5leH00qynOHRmmTl0Zpk5dGaZOXRmmTl0Zpk5dGaZ/T8GpfF1WY+sMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rbm.generate_image(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
